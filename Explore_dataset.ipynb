{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import codecs\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import print_lines, load_lines, load_conversations, extract_sentence_pairs, load_prepare_data, trim_rare_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\cornell movie-dialogs corpus'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "corpus = os.path.join(\"data\", corpus_name)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create formatted data file\n",
    "For convenience, weâ€™ll create a nicely formatted data file in which each line contains a tab-separated query sentence and a response sentence pair.\n",
    "\n",
    "The following functions facilitate the parsing of the raw movie_lines.txt data file.\n",
    "\n",
    "1. load_lines splits each line of the file into a dictionary of fields (lineID, characterID, movieID, character, text)\n",
    "2. load_conversations groups fields of lines from load_lines into conversations based on movie_conversations.txt\n",
    "3. extract_sentence_pairs extracts pairs of sentences from conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the two files from this dataset to create a sentence:response pair\n",
    "\n",
    "1. movie_lines: Format [lineID, characterID, movieID, character, text]\n",
    "2. movie_conversations: Format [character1ID, character2ID, movieID, utteranceIDs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View sample of movie_lines file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "print_lines(os.path.join(corpus, \"movie_lines.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert lines to a dict based on each field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus...\n"
     ]
    }
   ],
   "source": [
    "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "\n",
    "# Load lines and process conversations\n",
    "print(\"\\nProcessing corpus...\")\n",
    "lines = load_lines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group fields of lines from `load_lines` into conversations based on *movie_conversations.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "conversations = load_conversations(os.path.join(corpus, \"movie_conversations.txt\"),\n",
    "                                  lines, MOVIE_CONVERSATIONS_FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'character1ID': 'u0',\n",
       "  'character2ID': 'u2',\n",
       "  'movieID': 'm0',\n",
       "  'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\\n\",\n",
       "  'lines': [{'lineID': 'L194',\n",
       "    'characterID': 'u0',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'BIANCA',\n",
       "    'text': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n'},\n",
       "   {'lineID': 'L195',\n",
       "    'characterID': 'u2',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'CAMERON',\n",
       "    'text': \"Well, I thought we'd start with pronunciation, if that's okay with you.\\n\"},\n",
       "   {'lineID': 'L196',\n",
       "    'characterID': 'u0',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'BIANCA',\n",
       "    'text': 'Not the hacking and gagging and spitting part.  Please.\\n'},\n",
       "   {'lineID': 'L197',\n",
       "    'characterID': 'u2',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'CAMERON',\n",
       "    'text': \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"}]},\n",
       " {'character1ID': 'u0',\n",
       "  'character2ID': 'u2',\n",
       "  'movieID': 'm0',\n",
       "  'utteranceIDs': \"['L198', 'L199']\\n\",\n",
       "  'lines': [{'lineID': 'L198',\n",
       "    'characterID': 'u0',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'BIANCA',\n",
       "    'text': \"You're asking me out.  That's so cute. What's your name again?\\n\"},\n",
       "   {'lineID': 'L199',\n",
       "    'characterID': 'u2',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'CAMERON',\n",
       "    'text': 'Forget it.\\n'}]},\n",
       " {'character1ID': 'u0',\n",
       "  'character2ID': 'u2',\n",
       "  'movieID': 'm0',\n",
       "  'utteranceIDs': \"['L200', 'L201', 'L202', 'L203']\\n\",\n",
       "  'lines': [{'lineID': 'L200',\n",
       "    'characterID': 'u0',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'BIANCA',\n",
       "    'text': \"No, no, it's my fault -- we didn't have a proper introduction ---\\n\"},\n",
       "   {'lineID': 'L201',\n",
       "    'characterID': 'u2',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'CAMERON',\n",
       "    'text': 'Cameron.\\n'},\n",
       "   {'lineID': 'L202',\n",
       "    'characterID': 'u0',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'BIANCA',\n",
       "    'text': \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"},\n",
       "   {'lineID': 'L203',\n",
       "    'characterID': 'u2',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'CAMERON',\n",
       "    'text': 'Seems like she could get a date easy enough...\\n'}]},\n",
       " {'character1ID': 'u0',\n",
       "  'character2ID': 'u2',\n",
       "  'movieID': 'm0',\n",
       "  'utteranceIDs': \"['L204', 'L205', 'L206']\\n\",\n",
       "  'lines': [{'lineID': 'L204',\n",
       "    'characterID': 'u2',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'CAMERON',\n",
       "    'text': 'Why?\\n'},\n",
       "   {'lineID': 'L205',\n",
       "    'characterID': 'u0',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'BIANCA',\n",
       "    'text': 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'},\n",
       "   {'lineID': 'L206',\n",
       "    'characterID': 'u2',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'CAMERON',\n",
       "    'text': \"That's a shame.\\n\"}]},\n",
       " {'character1ID': 'u0',\n",
       "  'character2ID': 'u2',\n",
       "  'movieID': 'm0',\n",
       "  'utteranceIDs': \"['L207', 'L208']\\n\",\n",
       "  'lines': [{'lineID': 'L207',\n",
       "    'characterID': 'u0',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'BIANCA',\n",
       "    'text': 'Gosh, if only we could find Kat a boyfriend...\\n'},\n",
       "   {'lineID': 'L208',\n",
       "    'characterID': 'u2',\n",
       "    'movieID': 'm0',\n",
       "    'character': 'CAMERON',\n",
       "    'text': 'Let me see what I can do.\\n'}]}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracts pairs of sentences from conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to new file\n",
    "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing newly formatted file...\n",
      "\n",
      "Sample lines from file:\n",
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\r\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\n\"\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\r\\n\"\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\r\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\r\\n'\n",
      "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\r\\n\"\n",
      "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "# Write new csv file\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in extract_sentence_pairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "\n",
    "# Print a sample of lines\n",
    "print(\"\\nSample lines from file:\")\n",
    "print_lines(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and trim data\n",
    "Our next order of business is to create a vocabulary and load query/response sentence pairs into memory.\n",
    "\n",
    "Note that we are dealing with sequences of words, which do not have an implicit mapping to a discrete numerical space. Thus, we must create one by mapping each unique word that we encounter in our dataset to an index value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assemble our vocabulary and query/response sentence pairs. Before we are ready to use this data, we must perform some preprocessing.\n",
    "\n",
    "First, we must convert the Unicode strings to ASCII using unicodeToAscii. Next, we should convert all letters to lowercase and trim all non-letter characters except for basic punctuation (normalizeString). Finally, to aid in training convergence, we will filter out sentences with length greater than the MAX_LENGTH threshold (filterPairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 221282 sentence pairs\n",
      "Trimmed to 64271 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 18008\n",
      "\n",
      "pairs:\n",
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = load_prepare_data(corpus, corpus_name, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tactic that is beneficial to achieving faster convergence during training is trimming rarely used words out of our vocabulary. Decreasing the feature space will also soften the difficulty of the function that the model must learn to approximate. We will do this as a two-step process:\n",
    "\n",
    "Trim words used under MIN_COUNT threshold using the voc.trim function.\n",
    "Filter out pairs with trimmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 3    # Minimum word count threshold for trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7823 / 18005 = 0.4345\n",
      "Trimmed from 64271 pairs to 53165, 0.8272 of total\n"
     ]
    }
   ],
   "source": [
    "# Trim voc and pairs\n",
    "pairs = trim_rare_words(voc, pairs, MIN_COUNT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
